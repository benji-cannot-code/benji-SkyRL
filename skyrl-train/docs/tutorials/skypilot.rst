Launching with Skypilot
=====================================

Skypilot
-------

`SkyPilot <https://docs.skypilot.co/en/latest>`_ is a system to run, manage,
and scale AI workloads on any infrastructure. It handles all of the logic of
provisioning, setting up, and tearing down clusters on any cloud.

Setup Skypilot
--------------

SkyPilot works across AWS, GCP, Azure, OCI, Lambda, Nebius, and more. The
commands below use AWS as a concrete example. You can swap ``infra`` and accelerator
names (for example ``infra: nebius`` with ``accelerators: L40:2``) to target a
different cloud.

.. code-block:: bash

    git clone https://github.com/NovaSky-AI/SkyRL.git
    cd SkyRL

    # Install uv if you have not already.
    curl -LsSf https://astral.sh/uv/install.sh | sh

    # Create a dedicated env for the SkyPilot CLI and install the stable build.
    uv venv --python 3.12 --seed
    source .venv/bin/activate
    uv pip install "skypilot[aws]"  # replace aws with the clouds you plan to use

    # Validate your configuration.
    sky check aws

Run SkyRL with SkyPilot
-----------------------

To interact with SkyPilot we need to write a yaml defining the job. For the
full set of options allowed see `SkyPilot docs <https://docs.skypilot.co/en/latest/reference/yaml-spec.html>`_.

.. note::

   Docker image ``erictang000/skyrl-train-ray-2.48.0-py3.12-cu12.8`` depends on
   features that have not been released in the stable SkyPilot builds as of
   October 8, 2025. Until SkyPilot publishes v0.10.3 or later, rely on the
   default base image resolved by SkyPilot and install extra dependencies inside
   ``setup``. See `skypilot-org/skypilot#7181 <https://github.com/skypilot-org/skypilot/pull/7181>`_.

.. code-block:: yaml

    # Run this from the repository root after cloning SkyRL.
    resources:
      infra: aws           # replace this with what cloud you want to launch on
      accelerators: L40S:4 # 4x 48 GB GPUs; adjust to the SKU that matches your quota
      memory: 64+          # every node has at least 64 GB memory
      ports: 6479          # expose port for ray dashboard
    # check the skypilot documentation for what clouds network tier is supported.
    #  network_tier: best # when using multiple nodes, communication can become a bottleneck

    num_nodes: 1         # cluster size

    # --------------- Work Directory Synchronization (workdir) ---------------
    # Defines the local working directory to be synchronized to the remote cluster.
    # Here, '.' means synchronizing the directory where the sky submit command is currently run.
    # For this script, run the command from the repository root after cloning SkyRL.
    workdir: .

    # --------------- (secrets) ---------------
    secrets:
      WANDB_API_KEY: null
      # Either add your key here, or add it when launching with sky launch .... --secret WANDB_API_KEY="1234"

    # --------------- File Mounts/Data Upload (file_mounts) ---------------
    # If your dataset (gsm8k folder) is local, it needs to be uploaded to the
    # remote cluster. We prepare the data in the setup step here. Uncomment these
    # lines if you have the data locally already.

    #file_mounts:
    #  # Remote path (relative to remote user's home directory): Local path
    #  # /remote/dir1/file: /local/dir1/file
    #  data/gsm8k: ~/data/gsm8k

    # --------------- Environment variables ---------------
    envs:
      LOGGER: "wandb"  # change to "console" to print to stdout

      INFERENCE_BACKEND: "vllm"
      # INFERENCE_BACKEND: "sglang"

    # The quickstart configuration assumes 4x 48 GB GPUs (e.g., AWS g6.12xlarge with L40S).
    # If you only have lower-memory accelerators available, follow the Quickstart guidance
    # to switch to a smaller policy model (for example Qwen/Qwen2.5-0.5B-Instruct) and reduce
    # the micro batch sizes accordingly.


    # --------------- Environment Setup (setup) ---------------
    # Commands run on each node of the remote cluster to set up the environment (e.g., install dependencies). These are run directly inside Docker.
    setup: |
      cd skyrl-train
      uv venv --python 3.12 --seed
      source .venv/bin/activate
      uv sync --extra vllm
      uv pip install wandb
      uv run -- python examples/gsm8k/gsm8k_dataset.py --output_dir $HOME/data/gsm8k

    # If you already have processed the data locally, delete the above line

    # --------------- Run Command (run) ---------------
    # The actual task commands to be executed on the remote cluster.
    # This script will first start the Ray cluster (different ray start commands are executed on Head and Worker nodes).
    # Then, your training script will only be run on the Head node (SKYPILOT_NODE_RANK == 0).
    run: |
      set -euo pipefail

      cd skyrl-train
      source .venv/bin/activate

      TMP_DIR="$HOME/skyrl-tmp"
      mkdir -p "$TMP_DIR"
      export TMPDIR="$TMP_DIR"

      read -r head_ip _ <<< "$SKYPILOT_NODE_IPS"
      DATA_DIR="$HOME/data/gsm8k"

      # Login to Weights & Biases once the secrets are available.
      uv run -- python3 -c "import wandb; wandb.login(relogin=True, key='$WANDB_API_KEY')"

      wait_for_ray() {
        local address=$1
        for _ in $(seq 1 24); do
          if ray status --address "$address" >/dev/null 2>&1; then
            return 0
          fi
          sleep 5
        done
        echo "Ray cluster at $address failed to become ready" >&2
        return 1
      }

      export RAY_RUNTIME_ENV_HOOK=ray._private.runtime_env.uv_runtime_env_hook.hook
      if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
        if ! ray status --address 127.0.0.1:6479 >/dev/null 2>&1; then
          ray start --head --disable-usage-stats --port 6479
        fi
        wait_for_ray 127.0.0.1:6479
        uv run --isolated --extra "$INFERENCE_BACKEND" -m skyrl_train.entrypoints.main_base \
          data.train_data="['${DATA_DIR}/train.parquet']" \
          data.val_data="['${DATA_DIR}/validation.parquet']" \
          trainer.algorithm.advantage_estimator="grpo" \
          trainer.policy.model.path="Qwen/Qwen2.5-1.5B-Instruct" \
          trainer.placement.colocate_all=true \
          trainer.strategy=fsdp2 \
          trainer.placement.policy_num_gpus_per_node="$SKYPILOT_NUM_GPUS_PER_NODE" \
          trainer.placement.ref_num_gpus_per_node="$SKYPILOT_NUM_GPUS_PER_NODE" \
          trainer.placement.ref_num_nodes="$SKYPILOT_NUM_NODES" \
          trainer.placement.policy_num_nodes="$SKYPILOT_NUM_NODES" \
          generator.num_inference_engines="$SKYPILOT_NUM_GPUS_PER_NODE" \
          generator.inference_engine_tensor_parallel_size=1 \
          trainer.epochs=20 \
          trainer.eval_batch_size=1024 \
          trainer.eval_before_train=true \
          trainer.eval_interval=5 \
          trainer.update_epochs_per_batch=1 \
          trainer.train_batch_size=1024 \
          trainer.policy_mini_batch_size=256 \
          trainer.micro_forward_batch_size_per_gpu=64 \
          trainer.micro_train_batch_size_per_gpu=64 \
          trainer.ckpt_interval=10 \
          trainer.max_prompt_length=512 \
          generator.sampling_params.max_generate_length=1024 \
          trainer.policy.optimizer_config.lr=1.0e-6 \
          trainer.algorithm.use_kl_loss=true \
          generator.backend="$INFERENCE_BACKEND" \
          generator.run_engines_locally=true \
          generator.weight_sync_backend=nccl \
          generator.async_engine=true \
          generator.batched=true \
          environment.env_class=gsm8k \
          generator.n_samples_per_prompt=5 \
          generator.gpu_memory_utilization=0.8 \
          trainer.logger="$LOGGER" \
          trainer.project_name="gsm8k" \
          trainer.run_name="gsm8k_test" \
          trainer.resume_mode=null \
          trainer.ckpt_path="$HOME/ckpts/gsm8k_1.5B_ckpt"
      else
        if ! ray status --address "$head_ip:6479" >/dev/null 2>&1; then
          ray start --address "$head_ip:6479" --disable-usage-stats
        fi
        wait_for_ray "$head_ip:6479"
      fi

      echo "Node setup and Ray start script finished for rank ${SKYPILOT_NODE_RANK}."

You can launch this yaml with
``sky launch -c skyrl skyrl_train/examples/gsm8k/gsm8k-skypilot.yaml --secret WANDB_API_KEY="1234"``.
After it launches, you can easily access the cluster with ``ssh skyrl``. To
terminate the cluster simply run ``sky down skyrl``.

Launch Verification Views
-------------------------

Use the following reference views to confirm the environment and job status:

.. figure:: images/skypilot-dashboard.jpeg
   :alt: SkyPilot Dashboard showing the gsm8k cluster ready state
   :width: 80%

   SkyPilot Dashboard after ``sky launch`` reports the cluster as healthy.

.. figure:: images/skypilot-ray-logs.png
   :alt: Terminal logs from ``sky logs skyrl`` showing GRPO training progress
   :width: 80%

   ``sky logs`` streaming Ray task updates confirms Ray and SkyRL workers are active.

.. figure:: images/skypilot-wandb.jpeg
   :alt: Weights & Biases dashboard capturing the gsm8k_test run metrics
   :width: 80%

   Weights & Biases dashboard provides live metrics and checkpoints for the run.
